{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Simpleflow \u00b6 [![Pypi Status](https://badge.fury.io/py/simpleflow.png)](https://badge.fury.io/py/simpleflow) [![Build Status](https://travis-ci.org/botify-labs/simpleflow.svg?branch=main)](https://travis-ci.org/botify-labs/simpleflow) Simpleflow is a Python library that provides abstractions to write programs in the distributed dataflow paradigm . It coordinates the execution of distributed tasks with Amazon SWF . It relies on futures to describe the dependencies between tasks. A Future object models the asynchronous execution of a computation that may end. It tries to mimic the interface of the Python concurrent.futures library. Features \u00b6 Provides a Future abstraction to define dependencies between tasks. Define asynchronous tasks from callables. Handle workflows with Amazon SWF. Implement replay behavior like the Amazon Flow framework. Handle retry of tasks that failed. Automatically register decorated tasks. Encodes/decodes large fields to S3 objects transparently (aka \"jumbo fields\"). Handle the completion of a decision with more than 100 tasks. Provides a local executor to check a workflow without Amazon SWF (see simpleflow --local command). Provides decider and activity worker process for execution with Amazon SWF. Ships with the simpleflow command. simpleflow --help for more information about the commands it supports. You can read more in the Features section of the documentation. Overview \u00b6 Please read and even run the demo script to have a quick glance of simpleflow commands. To run the demo you will need to start decider and activity worker processes. Start a decider with: $ simpleflow decider.start --domain TestDomain --task-list test examples.basic.BasicWorkflow Start an activity worker with: $ simpleflow worker.start --domain TestDomain --task-list quickstart Then execute ./extras/demo . More information \u00b6 Read the main documentation at https://botify-labs.github.io/simpleflow/ .","title":"Intro"},{"location":"#simpleflow","text":"[![Pypi Status](https://badge.fury.io/py/simpleflow.png)](https://badge.fury.io/py/simpleflow) [![Build Status](https://travis-ci.org/botify-labs/simpleflow.svg?branch=main)](https://travis-ci.org/botify-labs/simpleflow) Simpleflow is a Python library that provides abstractions to write programs in the distributed dataflow paradigm . It coordinates the execution of distributed tasks with Amazon SWF . It relies on futures to describe the dependencies between tasks. A Future object models the asynchronous execution of a computation that may end. It tries to mimic the interface of the Python concurrent.futures library.","title":"Simpleflow"},{"location":"#features","text":"Provides a Future abstraction to define dependencies between tasks. Define asynchronous tasks from callables. Handle workflows with Amazon SWF. Implement replay behavior like the Amazon Flow framework. Handle retry of tasks that failed. Automatically register decorated tasks. Encodes/decodes large fields to S3 objects transparently (aka \"jumbo fields\"). Handle the completion of a decision with more than 100 tasks. Provides a local executor to check a workflow without Amazon SWF (see simpleflow --local command). Provides decider and activity worker process for execution with Amazon SWF. Ships with the simpleflow command. simpleflow --help for more information about the commands it supports. You can read more in the Features section of the documentation.","title":"Features"},{"location":"#overview","text":"Please read and even run the demo script to have a quick glance of simpleflow commands. To run the demo you will need to start decider and activity worker processes. Start a decider with: $ simpleflow decider.start --domain TestDomain --task-list test examples.basic.BasicWorkflow Start an activity worker with: $ simpleflow worker.start --domain TestDomain --task-list quickstart Then execute ./extras/demo .","title":"Overview"},{"location":"#more-information","text":"Read the main documentation at https://botify-labs.github.io/simpleflow/ .","title":"More information"},{"location":"contributing/","text":"Contributing guidelines \u00b6 In General \u00b6 PEP 8 , when sensible. Test ruthlessly. Write docs for new features. Even more important than Test-Driven Development, Human-Driven Development . In Particular \u00b6 Warning THE WHOLE SECTION IS OUT OF DATE. Questions, Feature Requests, Bug Reports, and Feedback ... should all be reported on the Github Issue Tracker . Setting Up for Local Development Fork simpleflow _ on Github. Clone your fork:: $ git clone git@github.com /botify-labs/simpleflow.git Make your virtualenv and install dependencies. If you have virtualenv and virtualenvwrapper_, run:: $ mkvirtualenv simpleflow $ cd simpleflow $ pip install -r requirements-dev.txt If you don't have virtualenv and virtualenvwrapper, you can install both using virtualenv-burrito . Git Branch Structure simpleflow used to have a separated devel branch but is now using only one, main branch main , that contains what will be released in the next version. This branch is (hopefully) always stable. Pull Requests Create a new local branch. :: $ git checkout -b name-of-feature Commit your changes. Write good commit messages . $ git commit -m \"Detailed commit message\" $ git push origin name-of-feature Before submitting a pull request, check the following: If the pull request adds functionality, it should be tested and the docs should be updated. The pull request should work on Python 2.7 and PyPy. Use tox to verify that it does. Submit a pull request to the main branch. Running tests To run all the tests in your current virtual environment: :: $ ./script/test This is what Travis CI does on each environment. If you want to simulate what Travis CI does, you can approach that by running a container from them: $ ./script/test-travis python2.7 $ ./script/test-travis pypy This can help you simulate locally what Travis CI would do. Be aware though that tests may fail depending on your OS, so Travis CI is the reference gate for the project. For instance, installing subprocess32 in a Pypy environment doesn't work on Mac OSX.","title":"Contributing"},{"location":"contributing/#contributing-guidelines","text":"","title":"Contributing guidelines"},{"location":"contributing/#in-general","text":"PEP 8 , when sensible. Test ruthlessly. Write docs for new features. Even more important than Test-Driven Development, Human-Driven Development .","title":"In General"},{"location":"contributing/#in-particular","text":"Warning THE WHOLE SECTION IS OUT OF DATE. Questions, Feature Requests, Bug Reports, and Feedback ... should all be reported on the Github Issue Tracker . Setting Up for Local Development Fork simpleflow _ on Github. Clone your fork:: $ git clone git@github.com /botify-labs/simpleflow.git Make your virtualenv and install dependencies. If you have virtualenv and virtualenvwrapper_, run:: $ mkvirtualenv simpleflow $ cd simpleflow $ pip install -r requirements-dev.txt If you don't have virtualenv and virtualenvwrapper, you can install both using virtualenv-burrito . Git Branch Structure simpleflow used to have a separated devel branch but is now using only one, main branch main , that contains what will be released in the next version. This branch is (hopefully) always stable. Pull Requests Create a new local branch. :: $ git checkout -b name-of-feature Commit your changes. Write good commit messages . $ git commit -m \"Detailed commit message\" $ git push origin name-of-feature Before submitting a pull request, check the following: If the pull request adds functionality, it should be tested and the docs should be updated. The pull request should work on Python 2.7 and PyPy. Use tox to verify that it does. Submit a pull request to the main branch. Running tests To run all the tests in your current virtual environment: :: $ ./script/test This is what Travis CI does on each environment. If you want to simulate what Travis CI does, you can approach that by running a container from them: $ ./script/test-travis python2.7 $ ./script/test-travis pypy This can help you simulate locally what Travis CI would do. Be aware though that tests may fail depending on your OS, so Travis CI is the reference gate for the project. For instance, installing subprocess32 in a Pypy environment doesn't work on Mac OSX.","title":"In Particular"},{"location":"development/","text":"Development \u00b6 Requirements \u00b6 CPython 3.7+ Pypy 3.7+ The codebase currently needs to be compatible with Python 3.7. Note about Pypy: all tests pass but some parts of the deciders might not work; Pypy support is mostly for activity workers where you need the performance boost. Development environment \u00b6 A Dockerfile is provided to help development on non-Linux machines. You can build a simpleflow image with: ./script/docker-build And use it with: ./script/docker-run It will then mount your current directory inside the container and pass the most relevant variables (your AWS_* credentials for instance). Running tests \u00b6 You can run tests with: ./script/test Any parameter passed to this script is propagated to the underlying call to py.test . This wrapper script sets some environment variables which control the behavior of simpleflow during tests: SIMPLEFLOW_CLEANUP_PROCESSES : set to \"yes\" in tests, so tests will clean up child processes after each test case. You can set it to an empty string ( \"\" ) or omit it if outside script/test if you want to debug things and take care of it yourself. SIMPLEFLOW_ENV : set to \"test\" in tests, which changes some constants to ease or speed up tests. SWF_CONNECTION_RETRIES : set to \"1\" in tests, which avoids having too many retries on the SWF API calls (5 by default in production). SIMPLEFLOW_VCR_RECORD_MODE : set to \"none\" in tests, which avoids running requests against the real SWF endpoints in tests. If you need to update cassettes, see tests/integration/README.md Reproducing CI failures \u00b6 Note: we're currently migrating from Travis to GitHub CI It might happen that a test fails on Travis and you want to reproduce locally. Travis has a helpful section in their docs about reproducing such issues. Since 2022, simpleflow builds run on 20.04 containers on the Travis infrastructure. So you can get close to the Travis setup with something like: docker run -it \\ -u focal \\ -e DEBIAN_FRONTEND=noninteractive \\ -e PYTHONDONTWRITEBYTECODE=true \\ -v $(pwd):/botify-labs/simpleflow \\ quay.io/travisci/travis-python /bin/bash Then you may want to follow your failed build commands to reproduce the errors. For instance on pypy builds the commands look like: sudo apt-get install ca-certificates libssl1.0.0 cd botify-labs/simpleflow source ~/virtualenv/pypy/bin/activate pip install . pip install -r requirements-dev.txt rm -rf build/ ./script/test -vv Release \u00b6 In order to release a new version, you\u2019ll need credentials on pypi.python.org for this software, as long as write access to this repository. Ask via an issue if needed. The release process is then automated behind a script: ./script/release","title":"Development"},{"location":"development/#development","text":"","title":"Development"},{"location":"development/#requirements","text":"CPython 3.7+ Pypy 3.7+ The codebase currently needs to be compatible with Python 3.7. Note about Pypy: all tests pass but some parts of the deciders might not work; Pypy support is mostly for activity workers where you need the performance boost.","title":"Requirements"},{"location":"development/#development-environment","text":"A Dockerfile is provided to help development on non-Linux machines. You can build a simpleflow image with: ./script/docker-build And use it with: ./script/docker-run It will then mount your current directory inside the container and pass the most relevant variables (your AWS_* credentials for instance).","title":"Development environment"},{"location":"development/#running-tests","text":"You can run tests with: ./script/test Any parameter passed to this script is propagated to the underlying call to py.test . This wrapper script sets some environment variables which control the behavior of simpleflow during tests: SIMPLEFLOW_CLEANUP_PROCESSES : set to \"yes\" in tests, so tests will clean up child processes after each test case. You can set it to an empty string ( \"\" ) or omit it if outside script/test if you want to debug things and take care of it yourself. SIMPLEFLOW_ENV : set to \"test\" in tests, which changes some constants to ease or speed up tests. SWF_CONNECTION_RETRIES : set to \"1\" in tests, which avoids having too many retries on the SWF API calls (5 by default in production). SIMPLEFLOW_VCR_RECORD_MODE : set to \"none\" in tests, which avoids running requests against the real SWF endpoints in tests. If you need to update cassettes, see tests/integration/README.md","title":"Running tests"},{"location":"development/#reproducing-ci-failures","text":"Note: we're currently migrating from Travis to GitHub CI It might happen that a test fails on Travis and you want to reproduce locally. Travis has a helpful section in their docs about reproducing such issues. Since 2022, simpleflow builds run on 20.04 containers on the Travis infrastructure. So you can get close to the Travis setup with something like: docker run -it \\ -u focal \\ -e DEBIAN_FRONTEND=noninteractive \\ -e PYTHONDONTWRITEBYTECODE=true \\ -v $(pwd):/botify-labs/simpleflow \\ quay.io/travisci/travis-python /bin/bash Then you may want to follow your failed build commands to reproduce the errors. For instance on pypy builds the commands look like: sudo apt-get install ca-certificates libssl1.0.0 cd botify-labs/simpleflow source ~/virtualenv/pypy/bin/activate pip install . pip install -r requirements-dev.txt rm -rf build/ ./script/test -vv","title":"Reproducing CI failures"},{"location":"development/#release","text":"In order to release a new version, you\u2019ll need credentials on pypi.python.org for this software, as long as write access to this repository. Ask via an issue if needed. The release process is then automated behind a script: ./script/release","title":"Release"},{"location":"installation/","text":"Installation \u00b6 From the PyPI (recommended) \u00b6 $ pip install -U simpleflow From Source \u00b6 simpleflow is actively developed on Github . You can clone the public repo: $ git clone https://github.com/botify-labs/simpleflow Or download one of the following: tarball zipball Once you have the source, you can install it into your site-packages with :: $ python setup.py install","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#from-the-pypi-recommended","text":"$ pip install -U simpleflow","title":"From the PyPI (recommended)"},{"location":"installation/#from-source","text":"simpleflow is actively developed on Github . You can clone the public repo: $ git clone https://github.com/botify-labs/simpleflow Or download one of the following: tarball zipball Once you have the source, you can install it into your site-packages with :: $ python setup.py install","title":"From Source"},{"location":"license/","text":"License \u00b6 Copyright 2014 Greg Leclercq Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"Copyright 2014 Greg Leclercq Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"quickstart/","text":"Quickstart \u00b6 Let\u2019s take a simple example that computes the result of (x + 1) * 2 . You will find this example in examples/basic.py . We need to declare the functions as activities to make them available: import time from simpleflow import activity @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def increment ( x ): return x + 1 @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def double ( x ): return x * 2 @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def delay ( t , x ): time . sleep ( t ) return x And then define the workflow itself in a example.py file: from simpleflow import ( Workflow , futures , ) from .basic import delay , double , increment class BasicWorkflow ( Workflow ): name = \"basic\" version = \"example\" task_list = \"example\" def run ( self , x , t = 30 ): y = self . submit ( increment , x ) yy = self . submit ( delay , t , y ) z = self . submit ( double , y ) print ( f \"( { x } + 1) * 2 = { z . result } \" ) futures . wait ( yy , z ) return z . result Now check that the workflow works locally with an integer \"x\" and a wait value \"t\": $ simpleflow workflow.start --local examples.basic.BasicWorkflow --input '[1, 5]' (1 + 1) * 2 = 4 input is encoded in JSON format and can contain the list of positional arguments such as '[1, 1] or a dict with the args and kwargs keys such as {\"args\": [1], \"kwargs\": {}} , {\"kwargs\": {\"x\": 1}} , or '{\"args\": [1], \"kwargs\": {\"t\": 5}}' . Now that you are confident that the workflow should work, you can run it on Amazon SWF with the standalone command: $ simpleflow standalone --domain TestDomain examples.basic.BasicWorkflow --input '[1, 5]' The standalone command sets a unique task list and manage all the processes that are needed to execute the workflow: decider, activity worker, and a client that starts the workflow. It is very convenient for testing a workflow by executing it with SWF during the development steps or integration tests. Let\u2019s take a closer look to the workflow definition. It is a class that inherits from simpleflow.Workflow : class BasicWorkflow ( Workflow ): It defines 3 class attributes: name , the name of the SWF workflow type. version , the version of the SWF workflow type. It is currently provided only for labeling a workflow. task_list , the default task list (see it as a dynamically created queue) where decision tasks for this workflow will be sent. Any decider that listens on this task list can handle this workflow. This value can be overriden by the simpleflow commands and objects. It also implements the run method that takes two arguments: x and t=30 (i.e. t is optional and has the default value 30 ). These arguments are passed with the --input option. The run method describes the workflow and how its tasks should execute. Each time a decider takes a decision task, it executes again the run from the start. When the workflow execution starts, it evaluates y = self.submit(increment, x) for the first time. y holds a future in state PENDING . The execution continues with the line yy = self.submit(delay, t, y) . yy holds another future in state PENDING . This state means the task has not been scheduled. Now execution still continue in the run method with the line z = self.submit(double, y) . Here it needs the value of the y future to evaluate the double activity. As the execution cannot continue, the decider schedules the task increment . yy is not a dependency for any task, so it is not scheduled. Once the decider has scheduled the task for y , it sleeps and waits for an event to be woken up. This happens when the increment task completes. SWF schedules a decision task. A decider takes it and executes the BasicWorkflow.run method again from the start. It evaluates the line y = self.submit(increment, x) . The task associated with the y future has completed. Hence, y is in state FINISHED and contains the value 2 in y.result . The execution continues until it blocks. It goes by yy = self.submit(delay, t, y) that stays the same. Then it reaches z = self.submit(double, y) . It gets the value of y.result and z now holds a future in state PENDING . Execution reaches the line with the print . It blocks here because z.result is not available. The decider schedules the task backs by the z future: double(y) . The workflow execution continues so forth by evaluating the BasicWorkflow.run again from the start until it finishes.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"Let\u2019s take a simple example that computes the result of (x + 1) * 2 . You will find this example in examples/basic.py . We need to declare the functions as activities to make them available: import time from simpleflow import activity @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def increment ( x ): return x + 1 @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def double ( x ): return x * 2 @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def delay ( t , x ): time . sleep ( t ) return x And then define the workflow itself in a example.py file: from simpleflow import ( Workflow , futures , ) from .basic import delay , double , increment class BasicWorkflow ( Workflow ): name = \"basic\" version = \"example\" task_list = \"example\" def run ( self , x , t = 30 ): y = self . submit ( increment , x ) yy = self . submit ( delay , t , y ) z = self . submit ( double , y ) print ( f \"( { x } + 1) * 2 = { z . result } \" ) futures . wait ( yy , z ) return z . result Now check that the workflow works locally with an integer \"x\" and a wait value \"t\": $ simpleflow workflow.start --local examples.basic.BasicWorkflow --input '[1, 5]' (1 + 1) * 2 = 4 input is encoded in JSON format and can contain the list of positional arguments such as '[1, 1] or a dict with the args and kwargs keys such as {\"args\": [1], \"kwargs\": {}} , {\"kwargs\": {\"x\": 1}} , or '{\"args\": [1], \"kwargs\": {\"t\": 5}}' . Now that you are confident that the workflow should work, you can run it on Amazon SWF with the standalone command: $ simpleflow standalone --domain TestDomain examples.basic.BasicWorkflow --input '[1, 5]' The standalone command sets a unique task list and manage all the processes that are needed to execute the workflow: decider, activity worker, and a client that starts the workflow. It is very convenient for testing a workflow by executing it with SWF during the development steps or integration tests. Let\u2019s take a closer look to the workflow definition. It is a class that inherits from simpleflow.Workflow : class BasicWorkflow ( Workflow ): It defines 3 class attributes: name , the name of the SWF workflow type. version , the version of the SWF workflow type. It is currently provided only for labeling a workflow. task_list , the default task list (see it as a dynamically created queue) where decision tasks for this workflow will be sent. Any decider that listens on this task list can handle this workflow. This value can be overriden by the simpleflow commands and objects. It also implements the run method that takes two arguments: x and t=30 (i.e. t is optional and has the default value 30 ). These arguments are passed with the --input option. The run method describes the workflow and how its tasks should execute. Each time a decider takes a decision task, it executes again the run from the start. When the workflow execution starts, it evaluates y = self.submit(increment, x) for the first time. y holds a future in state PENDING . The execution continues with the line yy = self.submit(delay, t, y) . yy holds another future in state PENDING . This state means the task has not been scheduled. Now execution still continue in the run method with the line z = self.submit(double, y) . Here it needs the value of the y future to evaluate the double activity. As the execution cannot continue, the decider schedules the task increment . yy is not a dependency for any task, so it is not scheduled. Once the decider has scheduled the task for y , it sleeps and waits for an event to be woken up. This happens when the increment task completes. SWF schedules a decision task. A decider takes it and executes the BasicWorkflow.run method again from the start. It evaluates the line y = self.submit(increment, x) . The task associated with the y future has completed. Hence, y is in state FINISHED and contains the value 2 in y.result . The execution continues until it blocks. It goes by yy = self.submit(delay, t, y) that stays the same. Then it reaches z = self.submit(double, y) . It gets the value of y.result and z now holds a future in state PENDING . Execution reaches the line with the print . It blocks here because z.result is not available. The decider schedules the task backs by the z future: double(y) . The workflow execution continues so forth by evaluating the BasicWorkflow.run again from the start until it finishes.","title":"Quickstart"},{"location":"architecture/multiprocess/","text":"Multiprocess architecture \u00b6 Note This architecture is currently the recommended way to deploy simpleflow in production. If you\u2019re not familiar with the 3 standard roles around an SWF setup, go read the previous section about Standalone architecture . In this setup, the 3 roles are potentially distributed on different machines: A few notes about this schema: the workflow started role is generally behind a web app or an automated system depending on your use case ; it\u2019s pretty uncommon to have workflows launched manually via the simpleflow command-line. the activity workers can be distributed on many nodes, possibly with autoscaling mechanisms. the deciders on the other hand are usually only installed on a few machines, and don\u2019t need autoscaling.","title":"Multiprocess"},{"location":"architecture/multiprocess/#multiprocess-architecture","text":"Note This architecture is currently the recommended way to deploy simpleflow in production. If you\u2019re not familiar with the 3 standard roles around an SWF setup, go read the previous section about Standalone architecture . In this setup, the 3 roles are potentially distributed on different machines: A few notes about this schema: the workflow started role is generally behind a web app or an automated system depending on your use case ; it\u2019s pretty uncommon to have workflows launched manually via the simpleflow command-line. the activity workers can be distributed on many nodes, possibly with autoscaling mechanisms. the deciders on the other hand are usually only installed on a few machines, and don\u2019t need autoscaling.","title":"Multiprocess architecture"},{"location":"architecture/standalone/","text":"Standalone mode \u00b6 Warning This mode is well-suited for development but not for production, since it uses the local machine alone to take decisions and process activity tasks. In this mode, your local machine will act as: workflow starter : it submits the workflow to Amazon SWF (via the StartWorkflowExecution endpoint) decider : it polls SWF for decisions to take (via PollForDecisionTask ) then takes decisions with the workflow class you passed and submit decisions to SWF (via RespondDecisionTaskCompleted ) activity worker : it polls SWF for activity tasks to execute (via PollForActivityTask ) then processes the task and submit the result to SWF, either via RespondActivityTaskCompleted if successful, or via RespondActivityTaskFailed if not. Usually you\u2019ll want to tune the number of decider and worker processes that will be spawned by the command. The command will look like: simpleflow standalone --domain TestDomain \\ --nb-deciders 1 --nb-workers 1 \\ examples.basic.BasicWorkflow --input '[1]' Your process tree will look like: /code/simpleflow# ps auxf|grep simpleflow PID COMMAND 6 ... simpleflow standalone examples.basic.BasicWorkflow --input [1] 10 ... \\_ simpleflow Decider(payload=DeciderPoller.start, nb_children=1)[running] 12 ... | \\_ simpleflow DeciderPoller(task_list=basic-1234)[processing] 14 ... | \\_ simpleflow DeciderPoller(task_list=basic-1234)[deciding] 11 ... \\_ simpleflow Worker(payload=ActivityPoller.start, nb_children=1)[running] 13 ... \\_ simpleflow ActivityPoller(task_list=basic-1234)[polling] Under your command, you now see 2 subprocesses: a Decider supervisor process, that forks to one or more DeciderPoller processes a Worker supervisor process, that forks to one or more ActivityPoller processes Each poller then forks when doing real work related to SWF. Here we\u2019re in the middle of a decision for the workflow, and no activity task is running.","title":"Standalone"},{"location":"architecture/standalone/#standalone-mode","text":"Warning This mode is well-suited for development but not for production, since it uses the local machine alone to take decisions and process activity tasks. In this mode, your local machine will act as: workflow starter : it submits the workflow to Amazon SWF (via the StartWorkflowExecution endpoint) decider : it polls SWF for decisions to take (via PollForDecisionTask ) then takes decisions with the workflow class you passed and submit decisions to SWF (via RespondDecisionTaskCompleted ) activity worker : it polls SWF for activity tasks to execute (via PollForActivityTask ) then processes the task and submit the result to SWF, either via RespondActivityTaskCompleted if successful, or via RespondActivityTaskFailed if not. Usually you\u2019ll want to tune the number of decider and worker processes that will be spawned by the command. The command will look like: simpleflow standalone --domain TestDomain \\ --nb-deciders 1 --nb-workers 1 \\ examples.basic.BasicWorkflow --input '[1]' Your process tree will look like: /code/simpleflow# ps auxf|grep simpleflow PID COMMAND 6 ... simpleflow standalone examples.basic.BasicWorkflow --input [1] 10 ... \\_ simpleflow Decider(payload=DeciderPoller.start, nb_children=1)[running] 12 ... | \\_ simpleflow DeciderPoller(task_list=basic-1234)[processing] 14 ... | \\_ simpleflow DeciderPoller(task_list=basic-1234)[deciding] 11 ... \\_ simpleflow Worker(payload=ActivityPoller.start, nb_children=1)[running] 13 ... \\_ simpleflow ActivityPoller(task_list=basic-1234)[polling] Under your command, you now see 2 subprocesses: a Decider supervisor process, that forks to one or more DeciderPoller processes a Worker supervisor process, that forks to one or more ActivityPoller processes Each poller then forks when doing real work related to SWF. Here we\u2019re in the middle of a decision for the workflow, and no activity task is running.","title":"Standalone mode"},{"location":"features/canvas/","text":"Canvas Features \u00b6 Simpleflow handles task grouping above the SWF level, using canvas concepts from Celery : Chains and Groups. It also allows delayed function execution with FuncGroups. Chains \u00b6 A Chain links tasks together sequentially. from simpleflow import activity , Workflow from simpleflow.canvas import Chain @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def task_a (): return \"Something\" @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def task_b ( x ): return \"Something Else\" , x class AWorkflow ( Workflow ): # ... def run ( self , * args , ** kwargs ): futures = self . submit ( Chain ( task_a , ( task_b , 42 ))) print ( f \"Results: { futures . result } \" ) The future\u2019s result is the list of each chained task result once they are finished. The Chain.__init__ method takes a series of submittable objects or (submittable, ...args) tuples. The tasks can also be added after creating the chain, using the append method. from simpleflow import Workflow from simpleflow.canvas import Chain class AWorkflow ( Workflow ): # ... def run ( self , * args , ** kwargs ): chain = Chain () chain . append ( task_a ) chain . append ( task_b , x = 42 ) futures = self . submit ( chain ) print ( f \"Results: { futures . result } \" ) This allows the use of named arguments. Finally, appended tasks can directly be ActivityTask or WorkflowTask for maximum flexibility. Warning These are obviously too many ways of specifying tasks. Sending Results \u00b6 Each task in a chain can send its results to the next one, by using the send_results=True argument. The result is then added to the succeeding task\u2019s *args . Error Handling \u00b6 By default, an error in a task will break the chain and bubble the exception up. This can be controlled with several arguments: raises_on_failure (default: True) \u2014 bubble-up on failure break_on_failure (default: True) \u2014 break on failure bubbles_exception_on_failure (default: False) \u2014 in a sub-chain, report the not-raised failure to the upper chain See examples.canvas.CanvasWorkflow for what\u2019s happening in the different cases. raises_on_failure is propagated to the group\u2019s content if set. That is: * chain = Chain(raises_on_failure=False); chain.append(some_activity) will propagate raises_on_failure=False to some_activity ; * chain = Chain(); chain.append(some_activity); chain.raises_on_failure = False will not. The break_on_failure=False and send_results=True options are currently incompatible. Groups \u00b6 A Group represents independent tasks that are scheduled in parallel. from simpleflow import activity , Workflow from simpleflow.canvas import Group @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def task_a (): return \"Something\" @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def task_b ( x ): return \"Something Else\" , x class AWorkflow ( Workflow ): # ... def run ( self , * args , ** kwargs ): futures = self . submit ( Group ( task_a , ( task_b , 42 ))) print ( f \"Results: { futures . result } \" ) Defining a Group is similar to a Chain. The raises_on_failure and bubbles_exception_on_failure arguments are the same. An extra argument is max_parallel , which specifies how many tasks can be scheduled at a given time. FuncGroup \u00b6 A FuncGroup instance encapsulates a function called by the executor and returning a Chain or Group to execute. It can be seen as a barrier: in a Chain, the function will be called after the previous task. Warning Untested code from simpleflow import activity , Workflow from simpleflow.canvas import Chain , FuncGroup , Group @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def partition_data ( data_location ): # Partition a list of things to do into parallelizable sub-parts pass @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def execute_on_sub_part ( sub_part ): pass class AWorkflow ( Workflow ): # ... def run ( self , * args , ** kwargs ): chain = Chain ( send_result = True ) chain . append ( partition_data , data_location = \"s3://my_bucket/foo\" ) chain . append ( FuncGroup ( lambda parts : Group ( * [( execute_on_sub_part , sub_part ) for sub_part in parts ]))) Here, partition_data \u2019s result is passed to the FuncGroup lambda, which returns a Group parallelizing its execution. Advanced Uses \u00b6 No-result FuncGroup \u00b6 The function encapsulated in a FuncGroup must return a result; this is inconvenient when its job is limited to the workflow state, and must thus return an empty Group. Since this has been a long-standing policy, a new _allow_none argument relaxes this constraint. Warning This is a new experimental option: a better one might be to enforce that nothing is returned.","title":"Canvas"},{"location":"features/canvas/#canvas-features","text":"Simpleflow handles task grouping above the SWF level, using canvas concepts from Celery : Chains and Groups. It also allows delayed function execution with FuncGroups.","title":"Canvas Features"},{"location":"features/canvas/#chains","text":"A Chain links tasks together sequentially. from simpleflow import activity , Workflow from simpleflow.canvas import Chain @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def task_a (): return \"Something\" @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def task_b ( x ): return \"Something Else\" , x class AWorkflow ( Workflow ): # ... def run ( self , * args , ** kwargs ): futures = self . submit ( Chain ( task_a , ( task_b , 42 ))) print ( f \"Results: { futures . result } \" ) The future\u2019s result is the list of each chained task result once they are finished. The Chain.__init__ method takes a series of submittable objects or (submittable, ...args) tuples. The tasks can also be added after creating the chain, using the append method. from simpleflow import Workflow from simpleflow.canvas import Chain class AWorkflow ( Workflow ): # ... def run ( self , * args , ** kwargs ): chain = Chain () chain . append ( task_a ) chain . append ( task_b , x = 42 ) futures = self . submit ( chain ) print ( f \"Results: { futures . result } \" ) This allows the use of named arguments. Finally, appended tasks can directly be ActivityTask or WorkflowTask for maximum flexibility. Warning These are obviously too many ways of specifying tasks.","title":"Chains"},{"location":"features/canvas/#sending-results","text":"Each task in a chain can send its results to the next one, by using the send_results=True argument. The result is then added to the succeeding task\u2019s *args .","title":"Sending Results"},{"location":"features/canvas/#error-handling","text":"By default, an error in a task will break the chain and bubble the exception up. This can be controlled with several arguments: raises_on_failure (default: True) \u2014 bubble-up on failure break_on_failure (default: True) \u2014 break on failure bubbles_exception_on_failure (default: False) \u2014 in a sub-chain, report the not-raised failure to the upper chain See examples.canvas.CanvasWorkflow for what\u2019s happening in the different cases. raises_on_failure is propagated to the group\u2019s content if set. That is: * chain = Chain(raises_on_failure=False); chain.append(some_activity) will propagate raises_on_failure=False to some_activity ; * chain = Chain(); chain.append(some_activity); chain.raises_on_failure = False will not. The break_on_failure=False and send_results=True options are currently incompatible.","title":"Error Handling"},{"location":"features/canvas/#groups","text":"A Group represents independent tasks that are scheduled in parallel. from simpleflow import activity , Workflow from simpleflow.canvas import Group @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def task_a (): return \"Something\" @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def task_b ( x ): return \"Something Else\" , x class AWorkflow ( Workflow ): # ... def run ( self , * args , ** kwargs ): futures = self . submit ( Group ( task_a , ( task_b , 42 ))) print ( f \"Results: { futures . result } \" ) Defining a Group is similar to a Chain. The raises_on_failure and bubbles_exception_on_failure arguments are the same. An extra argument is max_parallel , which specifies how many tasks can be scheduled at a given time.","title":"Groups"},{"location":"features/canvas/#funcgroup","text":"A FuncGroup instance encapsulates a function called by the executor and returning a Chain or Group to execute. It can be seen as a barrier: in a Chain, the function will be called after the previous task. Warning Untested code from simpleflow import activity , Workflow from simpleflow.canvas import Chain , FuncGroup , Group @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def partition_data ( data_location ): # Partition a list of things to do into parallelizable sub-parts pass @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def execute_on_sub_part ( sub_part ): pass class AWorkflow ( Workflow ): # ... def run ( self , * args , ** kwargs ): chain = Chain ( send_result = True ) chain . append ( partition_data , data_location = \"s3://my_bucket/foo\" ) chain . append ( FuncGroup ( lambda parts : Group ( * [( execute_on_sub_part , sub_part ) for sub_part in parts ]))) Here, partition_data \u2019s result is passed to the FuncGroup lambda, which returns a Group parallelizing its execution.","title":"FuncGroup"},{"location":"features/canvas/#advanced-uses","text":"","title":"Advanced Uses"},{"location":"features/canvas/#no-result-funcgroup","text":"The function encapsulated in a FuncGroup must return a result; this is inconvenient when its job is limited to the workflow state, and must thus return an empty Group. Since this has been a long-standing policy, a new _allow_none argument relaxes this constraint. Warning This is a new experimental option: a better one might be to enforce that nothing is returned.","title":"No-result FuncGroup"},{"location":"features/command_line/","text":"Command Line \u00b6 Simpleflow comes with a simpleflow command-line utility that can be used to list workflows against SWF, boot decider or activity workers (with multiprocessing), and a few other goodies. List Workflow Executions \u00b6 $ simpleflow workflow.list TestDomain basic-example-1438722273 basic OPEN Workflow Execution Status \u00b6 $ simpleflow --header workflow.info TestDomain basic-example-1438722273 domain workflow_type.name workflow_type.version task_list workflow_id run_id tag_list execution_time input TestDomain basic example basic-example-1438722273 22QFVi362TnCh6BdoFgkQFlocunh24zEOemo1L12Yl5Go= 1.70 {'args': [1], 'kwargs': {}} Tasks Status \u00b6 You can check the status of the workflow execution with: $ simpleflow --header workflow.tasks DOMAIN WORKFLOW_ID [RUN_ID] --nb-tasks 3 $ simpleflow --header workflow.tasks TestDomain basic-example-1438722273 Tasks Last State Last State Time Scheduled Time examples.basic.increment scheduled 2015-08-04 23:04:34.510000 2015-08-04 23:04:34.510000 $ simpleflow --header workflow.tasks TestDomain basic-example-1438722273 Tasks Last State Last State Time Scheduled Time examples.basic.double completed 2015-08-04 23:06:19.200000 2015-08-04 23:06:17.738000 examples.basic.delay completed 2015-08-04 23:08:18.402000 2015-08-04 23:06:17.738000 examples.basic.increment completed 2015-08-04 23:06:17.503000 2015-08-04 23:04:34.510000 Profiling \u00b6 You can profile the execution of the workflow with: $ simpleflow --header workflow.profile TestDomain basic-example-1438722273 Task Last State Scheduled Time Scheduled Start Time Running End Percentage of total time activity-examples.basic.double-1 completed 2015-08-04 23:06 0.07 2015-08-04 23:06 1.39 2015-08-04 23:06 1.15 activity-examples.basic.increment-1 completed 2015-08-04 23:04 102.20 2015-08-04 23:06 0.79 2015-08-04 23:06 0.65 Controlling SWF access \u00b6 The SWF region is controlled by the environment variable AWS_DEFAULT_REGION . This variable comes from the legacy \"simple-workflow\" project. The option might be exposed through a --region option in the future (if you want that, please open an issue). The SWF domain is controlled by the --domain on most simpleflow commands. It can also be set via the SWF_DOMAIN environment variable. In case both are supplied, the command-line value takes precedence over the environment variable. Note that some simpleflow commands expect the domain to be passed as a positional argument. In that case the environment variable has no effect for now. The number of retries for accessing SWF can be controlled via SWF_CONNECTION_RETRIES (defaults to 5). The identity of SWF activity workers and deciders can be controlled via SIMPLEFLOW_IDENTITY which should be a JSON-serialized string representing { \"key\": \"value\" } pairs that adds up (or override) the basic identity provided by simpleflow. If some value is null in this JSON map, then the key is removed from the final SWF identity. Controlling log verbosity \u00b6 You can control log verbosity via the LOG_LEVEL environment variable. Default is INFO . For instance, the following command will start a decider with DEBUG logs: $ LOG_LEVEL=DEBUG simpleflow decider.start --domain TestDomain --task-list test examples.basic.BasicWorkflow","title":"Command Line"},{"location":"features/command_line/#command-line","text":"Simpleflow comes with a simpleflow command-line utility that can be used to list workflows against SWF, boot decider or activity workers (with multiprocessing), and a few other goodies.","title":"Command Line"},{"location":"features/command_line/#list-workflow-executions","text":"$ simpleflow workflow.list TestDomain basic-example-1438722273 basic OPEN","title":"List Workflow Executions"},{"location":"features/command_line/#workflow-execution-status","text":"$ simpleflow --header workflow.info TestDomain basic-example-1438722273 domain workflow_type.name workflow_type.version task_list workflow_id run_id tag_list execution_time input TestDomain basic example basic-example-1438722273 22QFVi362TnCh6BdoFgkQFlocunh24zEOemo1L12Yl5Go= 1.70 {'args': [1], 'kwargs': {}}","title":"Workflow Execution Status"},{"location":"features/command_line/#tasks-status","text":"You can check the status of the workflow execution with: $ simpleflow --header workflow.tasks DOMAIN WORKFLOW_ID [RUN_ID] --nb-tasks 3 $ simpleflow --header workflow.tasks TestDomain basic-example-1438722273 Tasks Last State Last State Time Scheduled Time examples.basic.increment scheduled 2015-08-04 23:04:34.510000 2015-08-04 23:04:34.510000 $ simpleflow --header workflow.tasks TestDomain basic-example-1438722273 Tasks Last State Last State Time Scheduled Time examples.basic.double completed 2015-08-04 23:06:19.200000 2015-08-04 23:06:17.738000 examples.basic.delay completed 2015-08-04 23:08:18.402000 2015-08-04 23:06:17.738000 examples.basic.increment completed 2015-08-04 23:06:17.503000 2015-08-04 23:04:34.510000","title":"Tasks Status"},{"location":"features/command_line/#profiling","text":"You can profile the execution of the workflow with: $ simpleflow --header workflow.profile TestDomain basic-example-1438722273 Task Last State Scheduled Time Scheduled Start Time Running End Percentage of total time activity-examples.basic.double-1 completed 2015-08-04 23:06 0.07 2015-08-04 23:06 1.39 2015-08-04 23:06 1.15 activity-examples.basic.increment-1 completed 2015-08-04 23:04 102.20 2015-08-04 23:06 0.79 2015-08-04 23:06 0.65","title":"Profiling"},{"location":"features/command_line/#controlling-swf-access","text":"The SWF region is controlled by the environment variable AWS_DEFAULT_REGION . This variable comes from the legacy \"simple-workflow\" project. The option might be exposed through a --region option in the future (if you want that, please open an issue). The SWF domain is controlled by the --domain on most simpleflow commands. It can also be set via the SWF_DOMAIN environment variable. In case both are supplied, the command-line value takes precedence over the environment variable. Note that some simpleflow commands expect the domain to be passed as a positional argument. In that case the environment variable has no effect for now. The number of retries for accessing SWF can be controlled via SWF_CONNECTION_RETRIES (defaults to 5). The identity of SWF activity workers and deciders can be controlled via SIMPLEFLOW_IDENTITY which should be a JSON-serialized string representing { \"key\": \"value\" } pairs that adds up (or override) the basic identity provided by simpleflow. If some value is null in this JSON map, then the key is removed from the final SWF identity.","title":"Controlling SWF access"},{"location":"features/command_line/#controlling-log-verbosity","text":"You can control log verbosity via the LOG_LEVEL environment variable. Default is INFO . For instance, the following command will start a decider with DEBUG logs: $ LOG_LEVEL=DEBUG simpleflow decider.start --domain TestDomain --task-list test examples.basic.BasicWorkflow","title":"Controlling log verbosity"},{"location":"features/continue_as_new/","text":"Continue As New \u00b6 In long-running workflow executions, the history can hit the 25,000-events hard SWF limit. This causes execution termination. To prevent this, the workflow can itself close the current execution and start another one by submitting self.continue_as_new(*args, **kwargs) : it is then restarted with a new run ID and an empty history. See examples/continue_as_new.py for a demonstration of this pattern: SWF_DOMAIN = TestDomain PYTHONPATH = $PWD simpleflow standalone \\ examples.continue_as_new.CANWorkflow \\ --nb-deciders 1 --nb-workers 1 In a real workflow, we would typically use steps to determine which activities have been executed and which ones need to run.","title":"Continue As New"},{"location":"features/continue_as_new/#continue-as-new","text":"In long-running workflow executions, the history can hit the 25,000-events hard SWF limit. This causes execution termination. To prevent this, the workflow can itself close the current execution and start another one by submitting self.continue_as_new(*args, **kwargs) : it is then restarted with a new run ID and an empty history. See examples/continue_as_new.py for a demonstration of this pattern: SWF_DOMAIN = TestDomain PYTHONPATH = $PWD simpleflow standalone \\ examples.continue_as_new.CANWorkflow \\ --nb-deciders 1 --nb-workers 1 In a real workflow, we would typically use steps to determine which activities have been executed and which ones need to run.","title":"Continue As New"},{"location":"features/error_handling/","text":"Error Handling \u00b6 Warning This feature is in beta mode and subject to changes. Any feedback is appreciated. What follows applies to the SWF executor; the local one only handles raises_on_failure . A workflow can declare a method to be called when a task or child workflow fails or times out. By default, errors are handled as follows: a retry attribute defines how many times the task or workflow is retried; it defaults to 0 (no retry) if too many retries were attempted, the raises_on_failure attribute is checked. If False, a future is returned with its exception set; if True, the workflow is aborted. Groups and Chains have raises_on_failure too, along with bubbles_exception_on_failure and (for Chains) break_on_failure , allowing for more fine-grained control. If defined, Workflow.on_task_failure is called before the Executor.default_failure_handling default error handling. It is passed a TaskFailureContext object with error details and can modify it or return a new TaskFailureContext instance. The TaskFailureContext currently have these members: a_task: failed ActivityTask or WorkflowTask task_name: activity or workflow name exception_class: TaskException or WorkflowException exception: raised exception (shortcut to future.exception); None for unfinished tasks retry_count: current retry count (0 for first retry) task_error: for a TaskFailed exception, name of the inner exception if available reason: TaskFailed.reason or str(exception) details: TaskFailed.details or None future: failed future event: quite opaque dict, experimental history: History object, experimental decision: described below retry_wait_timeout: ditto The TaskFailureContext.decision (nothing in common with SWF\u2019s decisions) should be set by on_task_failure . It can be: none: default value, continue with default handling (potential retries then abort) abort: use default abort strategy ignore: discard the exception, consider the task finished; the future\u2019s result may be user-modified cancel: mark the future as canceled retry: schedule the task again; its args and kwargs may be user-modified (well, the whole task) retry_later: schedule the task after retry_wait_timeout seconds, with args and kwargs potentially altered handled: on_task_failure somewhat handled the failure; use the future and task it has possibly modified (one strategy here is for on_task_failure to call the executor\u2019s default_failure_handling method, and make workflow-specific processing according to its return value) The Workflow.on_task_failure method is guaranteed to be called only once on a given replay, but may be called again with the same failure in subsequent replays: it must thus be idempotent.","title":"Error Handling"},{"location":"features/error_handling/#error-handling","text":"Warning This feature is in beta mode and subject to changes. Any feedback is appreciated. What follows applies to the SWF executor; the local one only handles raises_on_failure . A workflow can declare a method to be called when a task or child workflow fails or times out. By default, errors are handled as follows: a retry attribute defines how many times the task or workflow is retried; it defaults to 0 (no retry) if too many retries were attempted, the raises_on_failure attribute is checked. If False, a future is returned with its exception set; if True, the workflow is aborted. Groups and Chains have raises_on_failure too, along with bubbles_exception_on_failure and (for Chains) break_on_failure , allowing for more fine-grained control. If defined, Workflow.on_task_failure is called before the Executor.default_failure_handling default error handling. It is passed a TaskFailureContext object with error details and can modify it or return a new TaskFailureContext instance. The TaskFailureContext currently have these members: a_task: failed ActivityTask or WorkflowTask task_name: activity or workflow name exception_class: TaskException or WorkflowException exception: raised exception (shortcut to future.exception); None for unfinished tasks retry_count: current retry count (0 for first retry) task_error: for a TaskFailed exception, name of the inner exception if available reason: TaskFailed.reason or str(exception) details: TaskFailed.details or None future: failed future event: quite opaque dict, experimental history: History object, experimental decision: described below retry_wait_timeout: ditto The TaskFailureContext.decision (nothing in common with SWF\u2019s decisions) should be set by on_task_failure . It can be: none: default value, continue with default handling (potential retries then abort) abort: use default abort strategy ignore: discard the exception, consider the task finished; the future\u2019s result may be user-modified cancel: mark the future as canceled retry: schedule the task again; its args and kwargs may be user-modified (well, the whole task) retry_later: schedule the task after retry_wait_timeout seconds, with args and kwargs potentially altered handled: on_task_failure somewhat handled the failure; use the future and task it has possibly modified (one strategy here is for on_task_failure to call the executor\u2019s default_failure_handling method, and make workflow-specific processing according to its return value) The Workflow.on_task_failure method is guaranteed to be called only once on a given replay, but may be called again with the same failure in subsequent replays: it must thus be idempotent.","title":"Error Handling"},{"location":"features/jumbo_fields/","text":"Jumbo Fields \u00b6 Warning This feature is in beta mode and subject to changes. Any feedback is appreciated. For some use cases, you want to be able to have fields larger than what SWF accepts (which is maximum 32K bytes on the largest ones, input and result , and lower for some others, as documented here ). Simpleflow allows to transparently translate such fields to objects stored on AWS S3. The format is then the following: simpleflow+s3://jumbo-bucket/with/optional/prefix/5d7191af-[...]-cdd39a31ba61 5242880 Format \u00b6 The format provides a pseudo-S3 address as a first word. The simpleflow+s3:// prefix is here for implementation purposes, and may be extended later with other backends such as simpleflow+ssh or simpleflow+gs . The second word provides the length of the object in bytes, so a client parsing the SWF history can decide if it\u2019s worth it to pull/decode the object. For now jumbo fields are limited to 5MB in size. Simpleflow will optionally perform disk caching for this feature to avoid issuing too many queries to S3. The disk cache is enabled if you set the SIMPLEFLOW_ENABLE_DISK_CACHE environment variable. The resulting disk cache will be limited to 1GB, with an LRU eviction strategy. It uses Sqlite3 under the hood, and it\u2019s powered by the DiskCache library . Note that this cache used to be enabled by default, but it\u2019s not anymore, since it proved to slow things down under certain circumstances that we couldn\u2019t track down precisely. Configuration \u00b6 You have to configure an environment variable to tell simpleflow where to store things (which implicitly enables the feature by the way): SIMPLEFLOW_JUMBO_FIELDS_BUCKET=jumbo-bucket/with/optional/prefix And ensure your deciders and activity workers have access to this S3 bucket ( s3:GetObject and s3:PutObject should be enough, but please test it first). Warning on bucket name length The overhead of the signature format is maximum 91 chars at this point (fixed protocol and UUID width, and max 5M = 5242880 for the size part). So you should ensure that your bucket + directory is not longer than 256 - 91 = 165 chars, else you may not be able to get a working jumbo field signature for tiny fields. In that case stripping the signature would only break things down the road in unpredictable and hard to debug ways, so simpleflow will raise.","title":"Jumbo Fields"},{"location":"features/jumbo_fields/#jumbo-fields","text":"Warning This feature is in beta mode and subject to changes. Any feedback is appreciated. For some use cases, you want to be able to have fields larger than what SWF accepts (which is maximum 32K bytes on the largest ones, input and result , and lower for some others, as documented here ). Simpleflow allows to transparently translate such fields to objects stored on AWS S3. The format is then the following: simpleflow+s3://jumbo-bucket/with/optional/prefix/5d7191af-[...]-cdd39a31ba61 5242880","title":"Jumbo Fields"},{"location":"features/jumbo_fields/#format","text":"The format provides a pseudo-S3 address as a first word. The simpleflow+s3:// prefix is here for implementation purposes, and may be extended later with other backends such as simpleflow+ssh or simpleflow+gs . The second word provides the length of the object in bytes, so a client parsing the SWF history can decide if it\u2019s worth it to pull/decode the object. For now jumbo fields are limited to 5MB in size. Simpleflow will optionally perform disk caching for this feature to avoid issuing too many queries to S3. The disk cache is enabled if you set the SIMPLEFLOW_ENABLE_DISK_CACHE environment variable. The resulting disk cache will be limited to 1GB, with an LRU eviction strategy. It uses Sqlite3 under the hood, and it\u2019s powered by the DiskCache library . Note that this cache used to be enabled by default, but it\u2019s not anymore, since it proved to slow things down under certain circumstances that we couldn\u2019t track down precisely.","title":"Format"},{"location":"features/jumbo_fields/#configuration","text":"You have to configure an environment variable to tell simpleflow where to store things (which implicitly enables the feature by the way): SIMPLEFLOW_JUMBO_FIELDS_BUCKET=jumbo-bucket/with/optional/prefix And ensure your deciders and activity workers have access to this S3 bucket ( s3:GetObject and s3:PutObject should be enough, but please test it first). Warning on bucket name length The overhead of the signature format is maximum 91 chars at this point (fixed protocol and UUID width, and max 5M = 5242880 for the size part). So you should ensure that your bucket + directory is not longer than 256 - 91 = 165 chars, else you may not be able to get a working jumbo field signature for tiny fields. In that case stripping the signature would only break things down the road in unpredictable and hard to debug ways, so simpleflow will raise.","title":"Configuration"},{"location":"features/middleware/","text":"Middleware \u00b6 Warning This feature is in beta mode and subject to changes. Any feedback is appreciated. Presentation \u00b6 Simpleflow allows for the execution of functions before and after the execution of an Activity. To do this, you may pass functions as paths when running in standalone mode or when launching a worker ( worker.start command): simpleflow standalone --domain TestDomain \\ --nb-deciders 1 \\ --nb-workers 1 \\ --middleware-pre-execution module.path.pre.execution.function \\ --middleware-pre-execution module.path.pre.execution.second.function \\ --middleware-post-execution module.path.post.execution.function \\ myWorkflow \\ --input '{\"args\":[], \"kwargs\": {\"task_list\":\"test\"}}' The above example will execute two functions before each Activity code, and one after. The --middleware-pre-execution and --middleware-post-execution arguments are also accepted by the workflow.start command in --standalone mode. Writing a middleware function \u00b6 Middleware pre-execution functions receive the activity context. Middleware post-execution functions receive the activity context and result. def my_pre_execution_func(activity_context, **kwargs): pass def my_post_execution_func(activity_context, result, **kwargs): print(\"activity result:\", \"result\")","title":"Middleware"},{"location":"features/middleware/#middleware","text":"Warning This feature is in beta mode and subject to changes. Any feedback is appreciated.","title":"Middleware"},{"location":"features/middleware/#presentation","text":"Simpleflow allows for the execution of functions before and after the execution of an Activity. To do this, you may pass functions as paths when running in standalone mode or when launching a worker ( worker.start command): simpleflow standalone --domain TestDomain \\ --nb-deciders 1 \\ --nb-workers 1 \\ --middleware-pre-execution module.path.pre.execution.function \\ --middleware-pre-execution module.path.pre.execution.second.function \\ --middleware-post-execution module.path.post.execution.function \\ myWorkflow \\ --input '{\"args\":[], \"kwargs\": {\"task_list\":\"test\"}}' The above example will execute two functions before each Activity code, and one after. The --middleware-pre-execution and --middleware-post-execution arguments are also accepted by the workflow.start command in --standalone mode.","title":"Presentation"},{"location":"features/middleware/#writing-a-middleware-function","text":"Middleware pre-execution functions receive the activity context. Middleware post-execution functions receive the activity context and result. def my_pre_execution_func(activity_context, **kwargs): pass def my_post_execution_func(activity_context, result, **kwargs): print(\"activity result:\", \"result\")","title":"Writing a middleware function"},{"location":"features/program_tasks/","text":"Execution of Tasks as Programs \u00b6 The simpleflow.execute module allows to define functions that will be executed as a program. There are two modes: Convert the definition of a function as a command line. Execute a Python function in another process. Please refer to the simpleflow.tests.test_activity test module for further examples. Executing a function as a command line \u00b6 Let\u2019s take the example of ls : @execute . program () def ls (): pass Calling ls() in Python will execute the ls command. Here the purpose of the function definition is only to describe the command line. The reason for this is to map a call in a workflow definition to a program to execute on the command line. The program may be written in any language whereas the workflow definition is in Python. Executing a Python function in another process \u00b6 The rationale for this feature is to execute a function with another interpreter (such as pypy) or in another environment (virtualenv). @execute . python ( interpreter = \"pypy\" ) def inc ( xs ): return [ x + 1 for x in xs ] Calling inc(range(10)) in Python will execute the function with the pypy interpreter found in $PATH . Limitations \u00b6 The main limitation comes from the need to serialize the arguments and the return values to pass them as strings. Hence, all arguments and return values must be convertible into JSON values.","title":"Program Tasks"},{"location":"features/program_tasks/#execution-of-tasks-as-programs","text":"The simpleflow.execute module allows to define functions that will be executed as a program. There are two modes: Convert the definition of a function as a command line. Execute a Python function in another process. Please refer to the simpleflow.tests.test_activity test module for further examples.","title":"Execution of Tasks as Programs"},{"location":"features/program_tasks/#executing-a-function-as-a-command-line","text":"Let\u2019s take the example of ls : @execute . program () def ls (): pass Calling ls() in Python will execute the ls command. Here the purpose of the function definition is only to describe the command line. The reason for this is to map a call in a workflow definition to a program to execute on the command line. The program may be written in any language whereas the workflow definition is in Python.","title":"Executing a function as a command line"},{"location":"features/program_tasks/#executing-a-python-function-in-another-process","text":"The rationale for this feature is to execute a function with another interpreter (such as pypy) or in another environment (virtualenv). @execute . python ( interpreter = \"pypy\" ) def inc ( xs ): return [ x + 1 for x in xs ] Calling inc(range(10)) in Python will execute the function with the pypy interpreter found in $PATH .","title":"Executing a Python function in another process"},{"location":"features/program_tasks/#limitations","text":"The main limitation comes from the need to serialize the arguments and the return values to pass them as strings. Hence, all arguments and return values must be convertible into JSON values.","title":"Limitations"},{"location":"features/settings/","text":"Settings \u00b6 Simpleflow comes with a simpleflow.settings module which aims at configuring generic parameters not tied to a specific simpleflow command. Parameters can be specified: via an environment variable (higher precedence) via a python config module, specified by SIMPLEFLOW_SETTINGS_MODULE via defaults stored in simpleflow/settings/default.py (lowest precedence) For the rest of this document, we will take \"SIMPLEFLOW_IDENTITY\" as an example. It controls the identity of SWF deciders and workers as reported to the SWF API when polling for tasks. Configure a setting via an environment variable \u00b6 This is the simplest: $ export SIMPLEFLOW_IDENTITY='{\"hostname\":\"machine.example.net\"}' $ simpleflow info settings | grep IDENTITY SIMPLEFLOW_IDENTITY='{\"hostname\":\"machine.example.net\"}' Configure a setting via a custom module \u00b6 In that form, you will have to create a module that configures the settings you want first, like this: $ cat my/custom/module.py SIMPLEFLOW_IDENTITY = '{\"hostname\":\"harcoded.example.net\"}' $ export SIMPLEFLOW_SETTINGS_MODULE='my.custom.module' $ simpleflow info settings | grep IDENTITY SIMPLEFLOW_IDENTITY='{\"hostname\":\"harcoded.example.net\"}' Of course the example above is not very interesting since the value is hardcoded, but if you need some settings to be dynamically computed, this is how you can achieve it.","title":"Settings"},{"location":"features/settings/#settings","text":"Simpleflow comes with a simpleflow.settings module which aims at configuring generic parameters not tied to a specific simpleflow command. Parameters can be specified: via an environment variable (higher precedence) via a python config module, specified by SIMPLEFLOW_SETTINGS_MODULE via defaults stored in simpleflow/settings/default.py (lowest precedence) For the rest of this document, we will take \"SIMPLEFLOW_IDENTITY\" as an example. It controls the identity of SWF deciders and workers as reported to the SWF API when polling for tasks.","title":"Settings"},{"location":"features/settings/#configure-a-setting-via-an-environment-variable","text":"This is the simplest: $ export SIMPLEFLOW_IDENTITY='{\"hostname\":\"machine.example.net\"}' $ simpleflow info settings | grep IDENTITY SIMPLEFLOW_IDENTITY='{\"hostname\":\"machine.example.net\"}'","title":"Configure a setting via an environment variable"},{"location":"features/settings/#configure-a-setting-via-a-custom-module","text":"In that form, you will have to create a module that configures the settings you want first, like this: $ cat my/custom/module.py SIMPLEFLOW_IDENTITY = '{\"hostname\":\"harcoded.example.net\"}' $ export SIMPLEFLOW_SETTINGS_MODULE='my.custom.module' $ simpleflow info settings | grep IDENTITY SIMPLEFLOW_IDENTITY='{\"hostname\":\"harcoded.example.net\"}' Of course the example above is not very interesting since the value is hardcoded, but if you need some settings to be dynamically computed, this is how you can achieve it.","title":"Configure a setting via a custom module"},{"location":"features/signals/","text":"Signals \u00b6 Warning This feature is in beta mode and subject to changes. Any feedback is appreciated. Signals are handled via two methods: Workflow.signal and Workflow.wait_signal . They are currently only implemented with SWF. Signaling a workflow \u00b6 The Workflow.signal method sends a signal to one or several workflows. def run ( self ): # Send to self, parent and children future = self . submit ( self . signal ( \"signal_name\" , * args , ** kwargs )) # Send to specific workflow future = self . submit ( self . signal ( \"signal_name\" , workflow_id , run_id , * args , ** kwargs )) The future will be finished, its result being *args and **kwargs, as soon as at least one workflow has been signaled (including oneself). Waiting for a signal \u00b6 The Workflow.wait_signal returns a Future which result is the signal input. def run ( self ): future = self . submit ( self . wait_signal ( \"signal_name\" )) result = future . result Naturally, one isn\u2019t forced to wait on the future result: def run ( self ): my_signal = self . submit ( self . wait_signal ( \"signal_name\" )) if my_signal . finished : # Something happened self . process ( my_signal . result ) A workflow can choose to have signals not propagated to its parent by defining propagate_signals_to_parent = False . Limitations \u00b6 signals cannot be reset; they can be overwritten though (only the latest one count) derive from futures.Future to add the timestamp or counter and better names? This would bypass the \"reset\" issue too One way to handle recurrent signals is by using event_id \u2019s (available with Workflow.get_event_details ). For instance, when receiving a signal, check that a marker with the same name does not exist or is in the past (lower event ID); if so, the signal is new, so process it and create a marker. Implementation \u00b6 The swf.executor.signal method returns a swf.SignalTask instance. Its schedule method returns an ExternalWorkflowExecutionDecision containing the given signal, sent either to the running workflow or the specified one. This decision results in a SignalExternalWorkflowExecutionInitiated followed (if all\u2019s well) by a SignalExternalWorkflowExecutionInitiated in the sender\u2019s history; from these events, we create first a running, then a completed future. (It can also fail, for instance if the workflow doesn\u2019t exist.) The receiver gets a WorkflowExecutionSignaled with the signal name, input and external (i.e. sender) information. We may want every known workflow to be signaled too: if propagate=True is passed to Workflow.signal , the signal is propagated to the parent and children of the workflow. Since we propagate using SignalWorkflowExecution , not a decision, the target doesn\u2019t have the externalWorkflowExecution information; so we pass __workflow_id and __run_id in the input.","title":"Signals"},{"location":"features/signals/#signals","text":"Warning This feature is in beta mode and subject to changes. Any feedback is appreciated. Signals are handled via two methods: Workflow.signal and Workflow.wait_signal . They are currently only implemented with SWF.","title":"Signals"},{"location":"features/signals/#signaling-a-workflow","text":"The Workflow.signal method sends a signal to one or several workflows. def run ( self ): # Send to self, parent and children future = self . submit ( self . signal ( \"signal_name\" , * args , ** kwargs )) # Send to specific workflow future = self . submit ( self . signal ( \"signal_name\" , workflow_id , run_id , * args , ** kwargs )) The future will be finished, its result being *args and **kwargs, as soon as at least one workflow has been signaled (including oneself).","title":"Signaling a workflow"},{"location":"features/signals/#waiting-for-a-signal","text":"The Workflow.wait_signal returns a Future which result is the signal input. def run ( self ): future = self . submit ( self . wait_signal ( \"signal_name\" )) result = future . result Naturally, one isn\u2019t forced to wait on the future result: def run ( self ): my_signal = self . submit ( self . wait_signal ( \"signal_name\" )) if my_signal . finished : # Something happened self . process ( my_signal . result ) A workflow can choose to have signals not propagated to its parent by defining propagate_signals_to_parent = False .","title":"Waiting for a signal"},{"location":"features/signals/#limitations","text":"signals cannot be reset; they can be overwritten though (only the latest one count) derive from futures.Future to add the timestamp or counter and better names? This would bypass the \"reset\" issue too One way to handle recurrent signals is by using event_id \u2019s (available with Workflow.get_event_details ). For instance, when receiving a signal, check that a marker with the same name does not exist or is in the past (lower event ID); if so, the signal is new, so process it and create a marker.","title":"Limitations"},{"location":"features/signals/#implementation","text":"The swf.executor.signal method returns a swf.SignalTask instance. Its schedule method returns an ExternalWorkflowExecutionDecision containing the given signal, sent either to the running workflow or the specified one. This decision results in a SignalExternalWorkflowExecutionInitiated followed (if all\u2019s well) by a SignalExternalWorkflowExecutionInitiated in the sender\u2019s history; from these events, we create first a running, then a completed future. (It can also fail, for instance if the workflow doesn\u2019t exist.) The receiver gets a WorkflowExecutionSignaled with the signal name, input and external (i.e. sender) information. We may want every known workflow to be signaled too: if propagate=True is passed to Workflow.signal , the signal is propagated to the parent and children of the workflow. Since we propagate using SignalWorkflowExecution , not a decision, the target doesn\u2019t have the externalWorkflowExecution information; so we pass __workflow_id and __run_id in the input.","title":"Implementation"},{"location":"features/steps/","text":"Steps \u00b6 Steps allow to group activities at a logical level independently of the workflows, with interdependencies, skipped and forced replay, and much more. They are implemented as S3 objects and SWF markers. A step is defined mainly with: * a name * a submittable to run if not already done * an optional submittable to run otherwise * an optional list of step names depending on this step being run It can emit a signal (that happens whether the step runs or is skipped) and have the same bubbles_exception_on_failure attribute as a group. A step can be forced for several reasons: * at submission time, as determined by business logic * if it is a dependency of an executed step A step can also be explicitly skipped, although the \"force\" logic takes precedence over the \"skip\" one. Step names are hierarchical, using a dotted notation: forcing \"a\" will also force \"a.b.c\". \"*\" forces or skips everything. Using Steps \u00b6 Basic Example \u00b6 A workflow using steps should derive from the WorkflowStepMixin mixin. One then submits either self.step or a Step : from simpleflow import Workflow , activity , futures from simpleflow.step.workflow import WorkflowStepMixin @activity . with_attributes ( task_list = \"example\" , version = \"example\" , idempotent = True ) def do_something (): return \"this is something\" class AWorkflow ( Workflow , WorkflowStepMixin ): # ... def run ( self , ** context ): futures . wait ( self . submit ( self . step ( \"something\" , do_something ))) The first time AWorkflow is executed, the step \"something\" is run; next time the workflow is executed, the step is skipped. Creating a Step \u00b6 The Step.__init__ method is called with: * step_name: str : step name * activities: Submittable | SubmittableContainer : what to execute (generally a chain or a group, thus the plural) * force: bool=False : whether to force a step even if previously played * activities_if_step_already_done: Submittable | SubmittableContainer | None=None : what to execute if the step is skipped * emit_signal: bool=False : whether to emit a signal.{step_name} signal sent after the step is played/skipped * force_steps_if_executed: list[str] | None=None : dependent steps to play next if this one isn\u2019t skipped * bubbles_exception_on_failure: bool=False : flag applied to the chain encapsulating the step The WorkflowStepMixin.step method delegates to Step . Internals \u00b6 A submitted step is executed in different phases: * get the list of done steps * create a chain * if we should run the step (not done or forced): * determine the step was force-run or not * add dependent steps to the forced steps list * add to the chain: * add a SWF marker logging the step as scheduled * run the step * mark the step as done * add a SWF marker logging the step as completed * otherwise: * determine whether the step was force-skipped or not * add to the chain: * run the alternative submittable (\"activities_if_step_already_done\") * add a SWF marker logging the step as skipped * if configured so, add to the chain a SWF signal named \"step.{step_name}\" The chain is then submitted to the workflow. Done steps are tracked by S3 objects. We only list them, their content is not used. Customization \u00b6 The WorkflowStepMixin class supports several customizations; see for instance examples.step.CustomizedStepWorkflow . get_step_bucket() : return the bucket name, potentially with the region endpoint. Default: {SIMPLEFLOW_S3_HOST}/{STEP_BUCKET} get_step_path_prefix() : return the steps path prefix. Default: {workflow_id}/steps get_step_activity_params() : return supplemental activity parameters for GetStepsDoneTask and MarkStepDoneTask . A typical use is forcing a separate task list.","title":"Steps"},{"location":"features/steps/#steps","text":"Steps allow to group activities at a logical level independently of the workflows, with interdependencies, skipped and forced replay, and much more. They are implemented as S3 objects and SWF markers. A step is defined mainly with: * a name * a submittable to run if not already done * an optional submittable to run otherwise * an optional list of step names depending on this step being run It can emit a signal (that happens whether the step runs or is skipped) and have the same bubbles_exception_on_failure attribute as a group. A step can be forced for several reasons: * at submission time, as determined by business logic * if it is a dependency of an executed step A step can also be explicitly skipped, although the \"force\" logic takes precedence over the \"skip\" one. Step names are hierarchical, using a dotted notation: forcing \"a\" will also force \"a.b.c\". \"*\" forces or skips everything.","title":"Steps"},{"location":"features/steps/#using-steps","text":"","title":"Using Steps"},{"location":"features/steps/#basic-example","text":"A workflow using steps should derive from the WorkflowStepMixin mixin. One then submits either self.step or a Step : from simpleflow import Workflow , activity , futures from simpleflow.step.workflow import WorkflowStepMixin @activity . with_attributes ( task_list = \"example\" , version = \"example\" , idempotent = True ) def do_something (): return \"this is something\" class AWorkflow ( Workflow , WorkflowStepMixin ): # ... def run ( self , ** context ): futures . wait ( self . submit ( self . step ( \"something\" , do_something ))) The first time AWorkflow is executed, the step \"something\" is run; next time the workflow is executed, the step is skipped.","title":"Basic Example"},{"location":"features/steps/#creating-a-step","text":"The Step.__init__ method is called with: * step_name: str : step name * activities: Submittable | SubmittableContainer : what to execute (generally a chain or a group, thus the plural) * force: bool=False : whether to force a step even if previously played * activities_if_step_already_done: Submittable | SubmittableContainer | None=None : what to execute if the step is skipped * emit_signal: bool=False : whether to emit a signal.{step_name} signal sent after the step is played/skipped * force_steps_if_executed: list[str] | None=None : dependent steps to play next if this one isn\u2019t skipped * bubbles_exception_on_failure: bool=False : flag applied to the chain encapsulating the step The WorkflowStepMixin.step method delegates to Step .","title":"Creating a Step"},{"location":"features/steps/#internals","text":"A submitted step is executed in different phases: * get the list of done steps * create a chain * if we should run the step (not done or forced): * determine the step was force-run or not * add dependent steps to the forced steps list * add to the chain: * add a SWF marker logging the step as scheduled * run the step * mark the step as done * add a SWF marker logging the step as completed * otherwise: * determine whether the step was force-skipped or not * add to the chain: * run the alternative submittable (\"activities_if_step_already_done\") * add a SWF marker logging the step as skipped * if configured so, add to the chain a SWF signal named \"step.{step_name}\" The chain is then submitted to the workflow. Done steps are tracked by S3 objects. We only list them, their content is not used.","title":"Internals"},{"location":"features/steps/#customization","text":"The WorkflowStepMixin class supports several customizations; see for instance examples.step.CustomizedStepWorkflow . get_step_bucket() : return the bucket name, potentially with the region endpoint. Default: {SIMPLEFLOW_S3_HOST}/{STEP_BUCKET} get_step_path_prefix() : return the steps path prefix. Default: {workflow_id}/steps get_step_activity_params() : return supplemental activity parameters for GetStepsDoneTask and MarkStepDoneTask . A typical use is forcing a separate task list.","title":"Customization"},{"location":"features/swf_layer/","text":"SWF Object Layer \u00b6 simpleflow includes a swf module that is an object-oriented wrapper for the boto.swf library, used to access the Amazon Simple Workflow service. It aims to provide: Modelisation : Swf entities and concepts are to be manipulated through Models and QuerySets (any ressemblance with the Django API would not be a coincidence). High-level Events, History : A higher level of abstractions over SWF events and history . Events are implemented as stateful objects aware of their own state and possible transitions. History enhances the events flow description, and can be compiled to check its integrity and the activities statuses transitions. Decisions : Stateful abstractions above the SWF decision making system. Actors : SWF actors base implementation such as a Decider or an activity Worker from which the user can easily inherit to implement its own decision/processing model. Settings \u00b6 Bug The informations in this \"Settings\" section may be outdated, they need some love. Optional: region Settings are found respectively in: A credential file a .swf file in the user\u2019s home directory: [credentials] aws_access_key_id = <aws_access_key_id> aws_secret_access_key = <aws_secret_access_key> [defaults] region = us-east-1 The following environment variables - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY - AWS_DEFAULT_REGION If neither of the previous methods were used, you can still set the AWS credentials with swf.settings.set : >>> import swf.settings >>> swf . settings . set ( aws_access_key_id = \"MYAWSACCESSKEYID\" , ... aws_secret_access_key = \"MYAWSSECRETACCESSKEY\" , ... region = \"REGION\" ) # And then you\u2019re good to go... >>> queryset = DomainQuery () >>> queryset . all () [ Domain ( \"test1\" ), Domain ( \"test2\" )] Leaving these AWS API keys unspecified is fine, as Boto\u2019s credentials chain handler will discover them if present. Example usage \u00b6 Models \u00b6 Simple Workflow entities such as domains, workflow types, workflow executions and activity types are to be manipulated through swf using models . They are immutable swf objects representations providing an interface to objects attributes, local/remote objects synchronization and changes watch between these local and remote objects. # Models reside in the swf.models module >>> from swf.models import Domain , WorkflowType , WorkflowExecution , ActivityType # Once imported you\u2019re ready to create a local model instance >>> D = Domain ( \"my-test-domain-name\" , description = \"my-test-domain-description\" , retention_period = 60 ) # a Domain model local instance has been created, but nothing has been # sent to amazon. To do so, you have to save it. >>> D . save () Now you have a local Domain model object, and if no errors were raised, the save method have saved amazon-side. Sometimes you won\u2019t be able to know if the model you\u2019re manipulating has an upstream version: whether you\u2019ve acquired it through a queryset, or the remote object has been deleted for example. Fortunately, models are shipped with a set of functions to make sure your local objects keep synced and consistent. # Exists method lets you know if your model instance has an upstream version >>> D . exists True # What if changes have been made to the remote object? # synced and changes methods help ensuring local and remote models # are still synced and which changes have been made (in the case below # nothing has changed) >>> D . is_synced True >>> D . changes ModelDiff () What if your local object is out of sync? Models upstream method will fetch the remote version of your object and will build a new model instance using its attributes. >>> D . is_synced False >>> D . changes ModelDiff ( Difference ( \"status\" , \"REGISTERED\" , \"DEPRECATED\" ) ) # Let\u2019s pull the upstream version >>> D = D . upstream () >>> D . is_synced True >>> D . changes ModelDiff () QuerySets \u00b6 Models can be retrieved and instantiated via querysets. To continue over the django comparison, they\u2019re behaving like django managers. # As querying for models needs a valid connection to amazon service, # Queryset objects cannot act as classmethods proxy and have to be instantiated; # most of the time against a Domain model instance >>> from swf.querysets import DomainQuerySet , WorkflowTypeQuerySet # Domain querysets can be instantiated directly >>> domain_qs = DomainQuerySet () >>> workflow_domain = domain_qs . get ( \"MyTestDomain\" ) # and specific model retieved via .get method >>> workflow_qs = WorkflowTypeQuerySet ( workflow_domain ) # queryset built against model instance example >>> workflow_qs . all () [ WorkflowType ( \"TestType1\" ), WorkflowType ( \"TestType2\" ),] >>> workflow_qs . filter ( status = DEPRECATED ) [ WorkflowType ( \"DeprecatedType1\" ),] Events \u00b6 (coming soon) History \u00b6 (coming soon) Decisions \u00b6 (coming soon) Actors \u00b6 SWF workflows are based on a worker-decider pattern. Every actions in the flow is executed by a worker which runs supplied activity tasks. And every actions is the result of a decision taken by the decider reading the workflow events history and deciding what to do next. In order to ease the development of such workers and decider, swf exposes base classes for them located in the swf.actors submodule. An Actor must basically implement a start and stop method and can actually inherits from whatever runtime implementation you need: thread, gevent, multiprocess... class Actor ( ConnectedSWFObject ): def __init__ ( self , domain , task_list ) def start ( self ): def stop ( self ): Decider base class implements the core functionality of a swf decider: polling for decisions tasks, and sending back a decision task copleted decision. Every other special needs implementations are left up to the user. class Decider ( Actor ): def __init__ ( self , domain , task_list ) def complete ( self , task_token , decisions = None , execution_context = None ) def poll ( self , task_list = None , identity = None , maximum_page_size = None ) Worker base class implements the core functionality of a swf worker whoes role is to process activity tasks. It is basically able to poll for new activity tasks to process, send back a heartbeat to SWF service in order to let it know it hasn\u2019t failed or crashed, and to complete, fail or cancel the activity task it\u2019s processing. class ActivityWorker ( Actor ): def __init__ ( self , domain , task_list ) def cancel ( self , task_token , details = None ) def complete ( self , task_token , result = None ) def fail ( self , task_token , details = None , reason = None ) def heartbeat ( self , task_token , details = None ) def poll ( self , task_list = None , ** kwargs )","title":"SWF Object Layer"},{"location":"features/swf_layer/#swf-object-layer","text":"simpleflow includes a swf module that is an object-oriented wrapper for the boto.swf library, used to access the Amazon Simple Workflow service. It aims to provide: Modelisation : Swf entities and concepts are to be manipulated through Models and QuerySets (any ressemblance with the Django API would not be a coincidence). High-level Events, History : A higher level of abstractions over SWF events and history . Events are implemented as stateful objects aware of their own state and possible transitions. History enhances the events flow description, and can be compiled to check its integrity and the activities statuses transitions. Decisions : Stateful abstractions above the SWF decision making system. Actors : SWF actors base implementation such as a Decider or an activity Worker from which the user can easily inherit to implement its own decision/processing model.","title":"SWF Object Layer"},{"location":"features/swf_layer/#settings","text":"Bug The informations in this \"Settings\" section may be outdated, they need some love. Optional: region Settings are found respectively in: A credential file a .swf file in the user\u2019s home directory: [credentials] aws_access_key_id = <aws_access_key_id> aws_secret_access_key = <aws_secret_access_key> [defaults] region = us-east-1 The following environment variables - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY - AWS_DEFAULT_REGION If neither of the previous methods were used, you can still set the AWS credentials with swf.settings.set : >>> import swf.settings >>> swf . settings . set ( aws_access_key_id = \"MYAWSACCESSKEYID\" , ... aws_secret_access_key = \"MYAWSSECRETACCESSKEY\" , ... region = \"REGION\" ) # And then you\u2019re good to go... >>> queryset = DomainQuery () >>> queryset . all () [ Domain ( \"test1\" ), Domain ( \"test2\" )] Leaving these AWS API keys unspecified is fine, as Boto\u2019s credentials chain handler will discover them if present.","title":"Settings"},{"location":"features/swf_layer/#example-usage","text":"","title":"Example usage"},{"location":"features/swf_layer/#models","text":"Simple Workflow entities such as domains, workflow types, workflow executions and activity types are to be manipulated through swf using models . They are immutable swf objects representations providing an interface to objects attributes, local/remote objects synchronization and changes watch between these local and remote objects. # Models reside in the swf.models module >>> from swf.models import Domain , WorkflowType , WorkflowExecution , ActivityType # Once imported you\u2019re ready to create a local model instance >>> D = Domain ( \"my-test-domain-name\" , description = \"my-test-domain-description\" , retention_period = 60 ) # a Domain model local instance has been created, but nothing has been # sent to amazon. To do so, you have to save it. >>> D . save () Now you have a local Domain model object, and if no errors were raised, the save method have saved amazon-side. Sometimes you won\u2019t be able to know if the model you\u2019re manipulating has an upstream version: whether you\u2019ve acquired it through a queryset, or the remote object has been deleted for example. Fortunately, models are shipped with a set of functions to make sure your local objects keep synced and consistent. # Exists method lets you know if your model instance has an upstream version >>> D . exists True # What if changes have been made to the remote object? # synced and changes methods help ensuring local and remote models # are still synced and which changes have been made (in the case below # nothing has changed) >>> D . is_synced True >>> D . changes ModelDiff () What if your local object is out of sync? Models upstream method will fetch the remote version of your object and will build a new model instance using its attributes. >>> D . is_synced False >>> D . changes ModelDiff ( Difference ( \"status\" , \"REGISTERED\" , \"DEPRECATED\" ) ) # Let\u2019s pull the upstream version >>> D = D . upstream () >>> D . is_synced True >>> D . changes ModelDiff ()","title":"Models"},{"location":"features/swf_layer/#querysets","text":"Models can be retrieved and instantiated via querysets. To continue over the django comparison, they\u2019re behaving like django managers. # As querying for models needs a valid connection to amazon service, # Queryset objects cannot act as classmethods proxy and have to be instantiated; # most of the time against a Domain model instance >>> from swf.querysets import DomainQuerySet , WorkflowTypeQuerySet # Domain querysets can be instantiated directly >>> domain_qs = DomainQuerySet () >>> workflow_domain = domain_qs . get ( \"MyTestDomain\" ) # and specific model retieved via .get method >>> workflow_qs = WorkflowTypeQuerySet ( workflow_domain ) # queryset built against model instance example >>> workflow_qs . all () [ WorkflowType ( \"TestType1\" ), WorkflowType ( \"TestType2\" ),] >>> workflow_qs . filter ( status = DEPRECATED ) [ WorkflowType ( \"DeprecatedType1\" ),]","title":"QuerySets"},{"location":"features/swf_layer/#events","text":"(coming soon)","title":"Events"},{"location":"features/swf_layer/#history","text":"(coming soon)","title":"History"},{"location":"features/swf_layer/#decisions","text":"(coming soon)","title":"Decisions"},{"location":"features/swf_layer/#actors","text":"SWF workflows are based on a worker-decider pattern. Every actions in the flow is executed by a worker which runs supplied activity tasks. And every actions is the result of a decision taken by the decider reading the workflow events history and deciding what to do next. In order to ease the development of such workers and decider, swf exposes base classes for them located in the swf.actors submodule. An Actor must basically implement a start and stop method and can actually inherits from whatever runtime implementation you need: thread, gevent, multiprocess... class Actor ( ConnectedSWFObject ): def __init__ ( self , domain , task_list ) def start ( self ): def stop ( self ): Decider base class implements the core functionality of a swf decider: polling for decisions tasks, and sending back a decision task copleted decision. Every other special needs implementations are left up to the user. class Decider ( Actor ): def __init__ ( self , domain , task_list ) def complete ( self , task_token , decisions = None , execution_context = None ) def poll ( self , task_list = None , identity = None , maximum_page_size = None ) Worker base class implements the core functionality of a swf worker whoes role is to process activity tasks. It is basically able to poll for new activity tasks to process, send back a heartbeat to SWF service in order to let it know it hasn\u2019t failed or crashed, and to complete, fail or cancel the activity task it\u2019s processing. class ActivityWorker ( Actor ): def __init__ ( self , domain , task_list ) def cancel ( self , task_token , details = None ) def complete ( self , task_token , result = None ) def fail ( self , task_token , details = None , reason = None ) def heartbeat ( self , task_token , details = None ) def poll ( self , task_list = None , ** kwargs )","title":"Actors"},{"location":"features/tags/","text":"Tags \u00b6 Tags are free-form strings associated with a workflow execution by SWF, useful for filtering. They can be specified in several ways: when starting the workflow, with the --tags option of simpleflow workflow.start or tag_list argument of workflow_type.start_execution in the workflow class definition: either a tag_list class variable or a get_tag_list(cls, *args, **kwargs) class method Reusing a workflow parent\u2019s tag list is a common use case. The special variable Workflow.INHERIT_TAG_LIST allows this. from simpleflow import Workflow class MyChildWorkflow ( Workflow ): ... tag_list = Workflow . INHERIT_TAG_LIST","title":"Tags"},{"location":"features/tags/#tags","text":"Tags are free-form strings associated with a workflow execution by SWF, useful for filtering. They can be specified in several ways: when starting the workflow, with the --tags option of simpleflow workflow.start or tag_list argument of workflow_type.start_execution in the workflow class definition: either a tag_list class variable or a get_tag_list(cls, *args, **kwargs) class method Reusing a workflow parent\u2019s tag list is a common use case. The special variable Workflow.INHERIT_TAG_LIST allows this. from simpleflow import Workflow class MyChildWorkflow ( Workflow ): ... tag_list = Workflow . INHERIT_TAG_LIST","title":"Tags"},{"location":"features/task_lists/","text":"Task Lists \u00b6 Task lists are often used to route different tasks to specific groups of workers. The decider and activity task lists are distinct, even if they have the same name. For SWF activities, the task list is typically specified with @activity.with_attributes : from simpleflow import activity @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def double ( x ): return x * 2 Dynamic task lists are possible: from simpleflow import activity , Workflow def run_on ( func , task_list , ** kwargs ): return activity . with_attributes ( task_list = task_list , ** kwargs )( func ) def double ( x ): return x * 2 class MyWorkflow ( Workflow ): ... def run ( self , x , task_list , * args , ** kwargs ): result = self . submit ( run_on ( double , task_list ), x ) . result print ( f \"Result: { result } \" ) [ screen0 ] $ simpleflow decider.start examples.dyn_task_list.BasicWorkflow --task-list foo-decider [ screen1 ] $ simpleflow worker.start --task-list foo-worker [ screen2 ] $ simpleflow workflow.start examples.dyn_task_list.BasicWorkflow --task-list foo-decider --input '{\"args\": [3, \"foo-worker\"]}' For SWF workflows, a static task list is usually defined as a class variable in the Workflow subclass. Dynamic task lists are implemented by a get_task_list class method: from simpleflow import Workflow class MyWorkflow ( Workflow ): ... @classmethod def get_task_list ( cls , task_list , * args , ** kwargs ): return task_list def run ( self , x , task_list , * args , ** kwargs ): ... In this example, task_list is a mandatory workflow argument; a more realistic case would use a kwarg .","title":"Task Lists"},{"location":"features/task_lists/#task-lists","text":"Task lists are often used to route different tasks to specific groups of workers. The decider and activity task lists are distinct, even if they have the same name. For SWF activities, the task list is typically specified with @activity.with_attributes : from simpleflow import activity @activity . with_attributes ( task_list = \"quickstart\" , version = \"example\" ) def double ( x ): return x * 2 Dynamic task lists are possible: from simpleflow import activity , Workflow def run_on ( func , task_list , ** kwargs ): return activity . with_attributes ( task_list = task_list , ** kwargs )( func ) def double ( x ): return x * 2 class MyWorkflow ( Workflow ): ... def run ( self , x , task_list , * args , ** kwargs ): result = self . submit ( run_on ( double , task_list ), x ) . result print ( f \"Result: { result } \" ) [ screen0 ] $ simpleflow decider.start examples.dyn_task_list.BasicWorkflow --task-list foo-decider [ screen1 ] $ simpleflow worker.start --task-list foo-worker [ screen2 ] $ simpleflow workflow.start examples.dyn_task_list.BasicWorkflow --task-list foo-decider --input '{\"args\": [3, \"foo-worker\"]}' For SWF workflows, a static task list is usually defined as a class variable in the Workflow subclass. Dynamic task lists are implemented by a get_task_list class method: from simpleflow import Workflow class MyWorkflow ( Workflow ): ... @classmethod def get_task_list ( cls , task_list , * args , ** kwargs ): return task_list def run ( self , x , task_list , * args , ** kwargs ): ... In this example, task_list is a mandatory workflow argument; a more realistic case would use a kwarg .","title":"Task Lists"}]}